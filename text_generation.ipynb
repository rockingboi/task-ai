{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Dependencies\n",
        "!pip install transformers datasets torch\n",
        "\n",
        "# Step 2: Load a Small Pre-installed Dataset (Wikitext-2)\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load Wikitext-2 dataset for training (raw version for better control)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# Combine all text data from the dataset into one string\n",
        "text_data = \"\\n\".join(dataset[\"text\"])\n",
        "\n",
        "print(f\"Sample Data:\\n{text_data[:500]}\")  # Display the first 500 characters\n",
        "\n",
        "# Step 3: Preprocess the Dataset (Tokenization)\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the entire text dataset\n",
        "tokens = tokenizer(text_data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "print(f\"Tokenized Example:\\n{tokens['input_ids'][:10]}\")  # Display a tokenized sample\n",
        "\n",
        "# Step 4: Load the GPT-2 Model\n",
        "from transformers import GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained GPT-2 model with a language modeling head\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Step 5: Train the Model (Fine-tuning on Wikitext-2)\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Prepare input data for training\n",
        "input_ids = tokens[\"input_ids\"].to(device)\n",
        "attention_mask = tokens[\"attention_mask\"].to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop: Fine-tune the model for a small number of epochs\n",
        "epochs = 3  # Can be increased for better results\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Clear gradients from the previous step\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "    loss = outputs.loss  # Calculate the loss\n",
        "    print(f\"Epoch {epoch + 1} | Loss: {loss.item()}\")\n",
        "\n",
        "    loss.backward()  # Backpropagate the loss\n",
        "    optimizer.step()  # Update model parameters\n",
        "\n",
        "print(\"\\nTraining Complete!\")\n",
        "\n",
        "# Step 6: Generate Text Based on User Input (Interactive Mode)\n",
        "model.eval()\n",
        "\n",
        "# Prompt the user to enter a text prompt\n",
        "user_prompt = input(\"Enter a prompt to generate text: \")\n",
        "\n",
        "# Tokenize the user input prompt\n",
        "input_ids = tokenizer(user_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Generate text using the fine-tuned model\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=100,  # Generate up to 100 tokens\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,  # Avoid repetition\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# Decode and display the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(f\"\\nGenerated Text:\\n{generated_text}\")\n",
        "\n",
        "# Step 7: Save the Fine-tuned Model (Optional)\n",
        "model.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "\n",
        "print(\"\\nModel and tokenizer saved to './fine_tuned_gpt2'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ1t1I6Z61Ha",
        "outputId": "686b89d9-083f-447f-fa8f-2f50eabb3f25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Sample Data:\n",
            "\n",
            " = Valkyria Chronicles III = \n",
            "\n",
            "\n",
            " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs\n",
            "Tokenized Example:\n",
            "tensor([[  198,   796,   569, 18354,  7496, 17740,  6711,   796,   220,   628,\n",
            "           198,  2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,\n",
            "           791, 47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,\n",
            "           254,   112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,\n",
            "           837,  6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,\n",
            "          1267,   837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,\n",
            "          6711,  2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,\n",
            "            31,  2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13,\n",
            "         44206,   329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,\n",
            "           287,  2869,   837,   340,   318,   262,  2368,   983,   287,   262,\n",
            "           569, 18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,\n",
            "           286, 16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,\n",
            "           663, 27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,\n",
            "           983,   290,  5679,   262,   366, 17871,  5321,   366,   837,   257,\n",
            "         23634,  2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,\n",
            "           262,  5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,\n",
            "           290,   389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,\n",
            "           265,    88, 12552,   366,   764,   220,   628,   383,   983,  2540,\n",
            "          2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
            "           262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
            "          2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
            "           340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
            "           983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
            "          4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
            "         43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
            "         18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
            "           317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
            "           983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
            "            77,   764,   220,   628,   632,  1138,   351,  3967,  4200,   287,\n",
            "          2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,  8830,\n",
            "          9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,   837,\n",
            "          1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,   614,\n",
            "           764,   632,   373,   635, 16573,   656, 15911,   290,   281,  2656,\n",
            "          2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,   569,\n",
            "         18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,  6711,\n",
            "           373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,   351,\n",
            "           262,   983,   705,    82,  9902,  8313,   373,  2716,   287,  1946,\n",
            "           764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,   351,\n",
            "           262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,   329,\n",
            "           262, 14047,   604,   764,   220,   628,   198,   796,   796,  3776,\n",
            "          1759,   796,   796,   220,   628,   198,  1081,   351,  2180,   569,\n",
            "         18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,  6711,\n",
            "           318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,   810,\n",
            "          1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,   636,\n",
            "           287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,   832,\n",
            "          9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,  2095,\n",
            "         31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046, 25037,\n",
            "           290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,  2137,\n",
            "         33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835, 14838,\n",
            "           355,  8739,   326,   460,   307, 12748, 28660,   832,   290,   302,\n",
            "         21542,   355,   484,   389, 14838,   764,   383,  6339,   284,  1123,\n",
            "          1621,  4067]])\n",
            "Epoch 1 | Loss: 3.703620433807373\n",
            "Epoch 2 | Loss: 3.4409642219543457\n",
            "Epoch 3 | Loss: 3.1615471839904785\n",
            "\n",
            "Training Complete!\n",
            "Enter a prompt to generate text: police\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "police, and the police department's own internal affairs unit.\n",
            "\n",
            "The investigation into the shooting began in January, when a man who had been shot in the head was found dead in his car. The man's family said he had suffered a gunshot wound to the chest and was taken to a hospital. Police said the man had a history of mental illness and had recently been diagnosed with schizophrenia.\n",
            "\n",
            "Model and tokenizer saved to './fine_tuned_gpt2'\n"
          ]
        }
      ]
    }
  ]
}