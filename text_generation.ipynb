{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPM5s+S23M/f0xhbbIyL1ga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockingboi/task-ai/blob/main/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Dependencies\n",
        "!pip install transformers datasets torch"
      ],
      "metadata": {
        "id": "8xdB-7DR5_Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load a Small Pre-installed Dataset (Wikitext-2)\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load Wikitext-2 dataset for training (raw version for better control)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "# Combine all text data from the dataset into one string\n",
        "text_data = \"\\n\".join(dataset[\"text\"])\n",
        "\n",
        "print(f\"Sample Data:\\n{text_data[:500]}\")  # Display the first 500 characters\n",
        "\n",
        "# Step 3: Preprocess the Dataset (Tokenization)\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the entire text dataset\n",
        "tokens = tokenizer(text_data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "print(f\"Tokenized Example:\\n{tokens['input_ids'][:10]}\")  # Display a tokenized sample\n",
        "\n",
        "# Step 4: Load the GPT-2 Model\n",
        "from transformers import GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained GPT-2 model with a language modeling head\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Step 5: Train the Model (Fine-tuning on Wikitext-2)\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Prepare input data for training\n",
        "input_ids = tokens[\"input_ids\"].to(device)\n",
        "attention_mask = tokens[\"attention_mask\"].to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop: Fine-tune the model for a small number of epochs\n",
        "epochs = 3  # Can be increased for better results\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Clear gradients from the previous step\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "    loss = outputs.loss  # Calculate the loss\n",
        "    print(f\"Epoch {epoch + 1} | Loss: {loss.item()}\")\n",
        "\n",
        "    loss.backward()  # Backpropagate the loss\n",
        "    optimizer.step()  # Update model parameters\n",
        "\n",
        "print(\"\\nTraining Complete!\")\n",
        "\n",
        "# Step 6: Generate Text in a Continuous Loop Until User Stops\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "print(\"\\nType 'exit' or 'quit' to stop the text generation loop.\")\n",
        "\n",
        "# Store the last user prompt for regeneration\n",
        "last_prompt = None\n",
        "\n",
        "while True:\n",
        "    # Prompt the user for input or use the last prompt for regeneration\n",
        "    if last_prompt:\n",
        "        regenerate = input(\"Type 'regenerate' to generate again, or press Enter to enter a new prompt: \").strip().lower()\n",
        "        if regenerate == \"regenerate\":\n",
        "            user_prompt = last_prompt\n",
        "        else:\n",
        "            user_prompt = input(\"Enter a prompt to generate text: \")\n",
        "            if user_prompt.lower() in [\"exit\", \"quit\"]:\n",
        "                print(\"Exiting the text generation loop. Goodbye!\")\n",
        "                break\n",
        "            last_prompt = user_prompt  # Store the prompt for future regeneration\n",
        "    else:\n",
        "        user_prompt = input(\"Enter a prompt to generate text: \")\n",
        "        if user_prompt.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting the text generation loop. Goodbye!\")\n",
        "            break\n",
        "        last_prompt = user_prompt  # Store the prompt for future regeneration\n",
        "\n",
        "    # Tokenize the user input prompt\n",
        "    input_ids = tokenizer(user_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Generate text using the fine-tuned model\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,  # Generate up to 100 tokens\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,  # Avoid repetition\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode and display the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"\\nGenerated Text:\\n{generated_text}\\n\")\n",
        "\n",
        "# Step 7: Save the Fine-tuned Model (Optional)\n",
        "model.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "\n",
        "print(\"\\nModel and tokenizer saved to './fine_tuned_gpt2'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJac1Jm849G7",
        "outputId": "d9f8cafb-488b-4480-85f0-c95ff6c7a308"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Data:\n",
            "\n",
            " = Valkyria Chronicles III = \n",
            "\n",
            "\n",
            " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs\n",
            "Tokenized Example:\n",
            "tensor([[  198,   796,   569, 18354,  7496, 17740,  6711,   796,   220,   628,\n",
            "           198,  2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,\n",
            "           791, 47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,\n",
            "           254,   112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,\n",
            "           837,  6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,\n",
            "          1267,   837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,\n",
            "          6711,  2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,\n",
            "            31,  2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13,\n",
            "         44206,   329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,\n",
            "           287,  2869,   837,   340,   318,   262,  2368,   983,   287,   262,\n",
            "           569, 18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,\n",
            "           286, 16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,\n",
            "           663, 27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,\n",
            "           983,   290,  5679,   262,   366, 17871,  5321,   366,   837,   257,\n",
            "         23634,  2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,\n",
            "           262,  5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,\n",
            "           290,   389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,\n",
            "           265,    88, 12552,   366,   764,   220,   628,   383,   983,  2540,\n",
            "          2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
            "           262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
            "          2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
            "           340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
            "           983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
            "          4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
            "         43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
            "         18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
            "           317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
            "           983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
            "            77,   764,   220,   628,   632,  1138,   351,  3967,  4200,   287,\n",
            "          2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,  8830,\n",
            "          9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,   837,\n",
            "          1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,   614,\n",
            "           764,   632,   373,   635, 16573,   656, 15911,   290,   281,  2656,\n",
            "          2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,   569,\n",
            "         18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,  6711,\n",
            "           373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,   351,\n",
            "           262,   983,   705,    82,  9902,  8313,   373,  2716,   287,  1946,\n",
            "           764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,   351,\n",
            "           262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,   329,\n",
            "           262, 14047,   604,   764,   220,   628,   198,   796,   796,  3776,\n",
            "          1759,   796,   796,   220,   628,   198,  1081,   351,  2180,   569,\n",
            "         18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,  6711,\n",
            "           318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,   810,\n",
            "          1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,   636,\n",
            "           287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,   832,\n",
            "          9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,  2095,\n",
            "         31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046, 25037,\n",
            "           290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,  2137,\n",
            "         33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835, 14838,\n",
            "           355,  8739,   326,   460,   307, 12748, 28660,   832,   290,   302,\n",
            "         21542,   355,   484,   389, 14838,   764,   383,  6339,   284,  1123,\n",
            "          1621,  4067]])\n",
            "Epoch 1 | Loss: 3.7088868618011475\n",
            "Epoch 2 | Loss: 3.461548328399658\n",
            "Epoch 3 | Loss: 3.179793119430542\n",
            "\n",
            "Training Complete!\n",
            "\n",
            "Type 'exit' or 'quit' to stop the text generation loop.\n",
            "Enter a prompt to generate text: hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Text:\n",
            "hello, NY) – The New York Red Bulls have signed midfielder Jordan Morris to a one-year contract, the club announced today.\n",
            "\n",
            "Morris, 23, joins the Red Bull Arena as a member of the New England Revolution's academy. He joins a group of players who have been with the team since the beginning of last season, including midfielder Christian Pulisic, midfielder David Villa, defender Michael Bradley, and midfielder Michael Orozco. Morris, who has played in all 16 MLS\n",
            "\n",
            "Type 'regenerate' to generate again, or press Enter to enter a new prompt: regenerate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "hello, NY) – The New York Red Bulls have signed midfielder Jordan Morris to a one-year contract, the club announced today.\n",
            "\n",
            "Morris, 23, joins the Red Bull Arena as a member of the New England Revolution's academy. He joins a group of players who have been with the team since the beginning of last season, including midfielder Christian Pulisic, midfielder David Villa, defender Michael Bradley, and midfielder Michael Orozco. Morris, who has played in all 16 MLS\n",
            "\n"
          ]
        }
      ]
    }
  ]
}